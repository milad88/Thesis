{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milad88/Thesis/blob/master/TRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "c_DoVIRsYadx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import gc\n",
        "import itertools\n",
        "from math import isnan\n",
        "EpisodeStats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\"])\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "def var_shape(x):\n",
        "    try:\n",
        "        out = [k for k in x.shape]\n",
        "    except:\n",
        "        out = [1]\n",
        "    assert all(isinstance(a, int) for a in out), \\\n",
        "        \"shape function assumes that shape is fully known\"\n",
        "    return out\n",
        "\n",
        "def flatgrad(loss, var_list):\n",
        "    grads = tf.gradients(loss, var_list)\n",
        "    grads = [0. + 1e-16 if g is None else g for g in grads]\n",
        "    grads = np.concatenate([np.reshape(grad, [np.size(v)]) for (v, grad) in zip(var_list, grads)], 0)\n",
        "    return grads\n",
        "\n",
        "\n",
        "def gauss_log_prob(mu, logstd, x):\n",
        "    var = tf.exp(2 * logstd)\n",
        "    gp = -tf.square(x - mu) / (2 * var) - .5 * tf.log(tf.constant(2 * np.pi)) - logstd\n",
        "    return tf.reduce_sum(gp, [1])\n",
        "\n",
        "\n",
        "def gauss_prob(mu, std, xs):\n",
        "    var = std ** 2\n",
        "    return tf.exp(-tf.square(xs - mu) / (2 * var)) / (tf.sqrt(tf.constant(2 * np.pi)) * std)\n",
        "\n",
        "# KL divergence between two paramaterized guassian distributions\n",
        "def gauss_KL(mu1, std1, mu2, std2):\n",
        "    var1 = std1 ** 2\n",
        "    var2 = std2 ** 2\n",
        "\n",
        "    kl = tf.reduce_sum(tf.log(std2) - tf.log(std1) + (var1 + tf.square(mu1 - mu2)) / (2 * var2) - 0.5)\n",
        "\n",
        "    return kl\n",
        "\n",
        "\n",
        "def gauss_ent(mu, std):\n",
        "    h = tf.reduce_sum(tf.log(std) + tf.constant(0.5 * np.log(2 * np.pi * np.e), tf.float32))\n",
        "    return h\n",
        "\n",
        "\n",
        "def hessian_vec_bk(ys, xs, vs, grads=None):\n",
        "    \"\"\"Implements Hessian vector product using backward on backward AD.\n",
        "  Args:\n",
        "    ys: Loss function.\n",
        "    xs: Weights, list of tensors.\n",
        "    vs: List of tensors to multiply, for each weight tensor.\n",
        "  Returns:\n",
        "    Hv: Hessian vector product, same size, same shape as xs.\n",
        "  \"\"\"\n",
        "    # Validate the input\n",
        "\n",
        "    if type(xs) == list:\n",
        "        if len(vs) != len(xs):\n",
        "            raise ValueError(\"xs and vs must have the same length.\")\n",
        "\n",
        "    if grads is None:\n",
        "        grads = tf.gradients(ys, xs, gate_gradients=True)\n",
        "    return tf.gradients(grads, xs, vs, gate_gradients=True)\n",
        "\n",
        "\n",
        "\n",
        "def plot_stats(stats):\n",
        "    fig11 = plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.plot(np.ravel(stats))\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"loss per episode\")\n",
        "    plt.show(fig11)\n",
        "\n",
        "\n",
        "def plot_episode_stats(stats, smoothing_window=10, noshow=False):\n",
        "    # Plot the episode reward over time\n",
        "    fig2 = plt.figure(figsize=(10, 5))\n",
        "    #rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
        "    plt.plot(stats.episode_rewards)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Episode Reward (Smoothed)\")\n",
        "    plt.title(\"Episode Reward over Time (Smoothed over window size {})\".format(smoothing_window))\n",
        "    fig2.savefig('reward.png')\n",
        "    if noshow:\n",
        "        plt.close(fig2)\n",
        "    else:\n",
        "        plt.show(fig2)\n",
        "\n",
        "\n",
        "class SetFromFlat(object):\n",
        "\n",
        "    def __init__(self, session, var_list):\n",
        "        self.session = session\n",
        "        self.var_list = var_list\n",
        "        self.shapes = map(var_shape, var_list)\n",
        "        total_size = sum(np.prod(shape) for shape in self.shapes)\n",
        "        self.theta = theta = tf.placeholder(tf.float32, [total_size],name=\"theta_sff\")\n",
        "        start = 0\n",
        "        assigns = []\n",
        "        for (shape, v) in zip(self.shapes, self.var_list):\n",
        "            size = np.prod(shape)\n",
        "            assigns.append(tf.assign(v, tf.reshape(theta[start:start + size], shape)))\n",
        "            start += size\n",
        "\n",
        "        self.op = assigns\n",
        "\n",
        "    def __call__(self, theta):\n",
        "        self.session.run(self.op, feed_dict={self.theta: theta})\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    # Replay buffer for experience replay. Stores transitions.\n",
        "    def __init__(self):\n",
        "        self._data = namedtuple(\"ReplayBuffer\", [\"states\", \"actions\", \"next_states\", \"rewards\", \"dones\"])\n",
        "        self._data = self._data(states=[], actions=[], next_states=[], rewards=[], dones=[])\n",
        "        self.position = 0\n",
        "        self.capacity = batch_size * 4\n",
        "\n",
        "    def add_transition(self, state, action, next_state, reward, done):\n",
        "\n",
        "        if np.array(state).shape == (3, 1):\n",
        "            state = list(itertools.chain.from_iterable(state))\n",
        "\n",
        "        if np.array(next_state).shape == (3, 1):\n",
        "            next_state = list(itertools.chain.from_iterable(next_state))\n",
        "\n",
        "        if len(self._data.states) < self.capacity:\n",
        "            self._data.states.append(None)\n",
        "            self._data.actions.append(None)\n",
        "            self._data.next_states.append(None)\n",
        "            self._data.rewards.append(None)\n",
        "            self._data.dones.append(None)\n",
        "\n",
        "        self._data.states[self.position] = state\n",
        "        self._data.actions[self.position] = action\n",
        "        self._data.next_states[self.position] = next_state\n",
        "        self._data.rewards[self.position] = reward\n",
        "        self._data.dones[self.position] = done\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def next_batch(self, batch_size):\n",
        "\n",
        "        self.capacity = batch_size * 4\n",
        "        if batch_size >= len(self._data.states):\n",
        "            return np.array(self._data.states), np.array(self._data.actions), np.array(\n",
        "                self._data.next_states), np.array(self._data.rewards), np.array(self._data.dones)\n",
        "        batch_indices = np.random.choice(len(self._data.states), batch_size)\n",
        "        batch_states = np.array([self._data.states[i] for i in batch_indices])\n",
        "        batch_actions = np.array([self._data.actions[i] for i in batch_indices])\n",
        "        batch_next_states = np.array([self._data.next_states[i] for i in batch_indices])\n",
        "        batch_rewards = np.array([self._data.rewards[i] for i in batch_indices])\n",
        "        batch_dones = np.array([self._data.dones[i] for i in batch_indices])\n",
        "\n",
        "        return batch_states, batch_actions, batch_next_states, batch_rewards, batch_dones\n",
        "\n",
        "    #        if self.transition_size() > 5*batch_size:\n",
        "    #           self._data.states = self._data.states[-5*batch_size:]\n",
        "    #          self._data.actions = self._data.actions[-5*batch_size:]\n",
        "    #         self._data.next_states = self._data.next_states[-5*batch_size:]\n",
        "    #       self._data.dones = self._data.dones[-5*batch_size:]\n",
        "    #      self._data.rewards = self._data.rewards[-5*batch_size:]\n",
        "\n",
        "\n",
        "    def transition_size(self):\n",
        "        return len(self._data.states)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fuM3zSQoYdZT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "action_bound = 2.0\n",
        "\n",
        "class Actor_Net():\n",
        "  def __init__(self, num_actions, action_dim, name, action_bound, state_dim, learning_rate=0.01, batch_size=32):\n",
        "        # super().__init__(num_actions, name)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.action_bound = action_bound\n",
        "        self.action_dim = action_dim\n",
        "        self.state_dim = state_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.name = name\n",
        "        self._build_model(num_actions)\n",
        "\n",
        "  def _build_model(self, num_actions):\n",
        "\n",
        "        self.inp = tf.placeholder(shape=[None, self.state_dim], dtype=tf.float32, name=\"states\")\n",
        "        self.actions = tf.placeholder(shape=[None, self.action_dim], dtype=tf.float32, name=\"actions\")\n",
        "\n",
        "        self.advantage = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"advantages\")\n",
        "        self.old_mean = tf.placeholder(dtype=tf.float32, name=\"old_mean\")\n",
        "        self.old_sigma = tf.placeholder(dtype=tf.float32, name=\"old_sigma\")\n",
        "        self.p = tf.placeholder(tf.float32, name=\"p\")  # the vector\n",
        "\n",
        "        self.inpW = tf.Variable(tf.random_uniform([self.state_dim, 16], -0.5, 0.5))\n",
        "        self.inpB = tf.Variable(tf.constant(0.1, shape=[16]))\n",
        "        self.h1 = tf.nn.relu(tf.matmul(self.inp, self.inpW) + self.inpB)\n",
        "\n",
        "        self.h2W = tf.Variable(tf.random_uniform([16, 32], -0.5, 0.5))\n",
        "        self.h2B = tf.Variable(tf.constant(0.1, shape=[32]))\n",
        "        self.h2 = tf.nn.relu(tf.matmul(self.h1, self.h2W) + self.h2B)\n",
        "\n",
        "        self.h3W = tf.Variable(tf.random_uniform([32, 16], -0.5, 0.5))\n",
        "        self.h3B = tf.Variable(tf.constant(0.1, shape=[16]))\n",
        "        self.h3 = tf.nn.relu(tf.matmul(self.h2, self.h3W) + self.h3B)\n",
        "\n",
        "        self.h4W = tf.Variable(tf.random_uniform([16, self.action_dim], -0.5, 0.5))\n",
        "\n",
        "        self.outB = tf.Variable(tf.constant(0.01, shape=[self.action_dim]))\n",
        "\n",
        "        self.net_params = tf.trainable_variables(scope=self.name)\n",
        "\n",
        "        self.mean = tf.nn.tanh(tf.matmul(self.h3, self.h4W) + self.outB)\n",
        "        self.mean = self.mean * self.action_bound\n",
        "\n",
        "        self.sigma = tf.nn.relu(tf.matmul(self.h3, self.h4W) + self.outB)\n",
        "\n",
        "        self.net_params = tf.trainable_variables(scope=self.name)\n",
        "\n",
        "        self.sigma = tf.clip_by_value(t=self.sigma,\n",
        "                                      clip_value_min=0,\n",
        "                                      clip_value_max=tf.sqrt(self.action_bound))\n",
        "        self.scaled_out = tf.truncated_normal(mean=self.mean, stddev=self.sigma, shape=[self.action_dim])\n",
        "        self.prev_mean = 0.\n",
        "        self.prev_sigma = 1.\n",
        "        #self.cost = gauss_KL(self.mean, self.sigma, self.prev_mean, self.prev_sigma)\n",
        "        self.cost = tf.reduce_sum((gauss_prob(self.mean, self.sigma, self.scaled_out) * self.advantage) /\n",
        "                        (gauss_prob(self.prev_mean, self.prev_sigma, self.scaled_out)) + 1e-10)\n",
        "\n",
        "        self.grads = tf.gradients(self.cost, self.net_params)\n",
        "\n",
        "        self.shapes = [v.shape.as_list() for v in self.net_params]\n",
        "        #self.size_theta = np.sum([np.prod(shape) for shape in self.shapes])\n",
        "\n",
        "        tangents = []\n",
        "        start = 0\n",
        "        for shape in self.shapes:\n",
        "            size = np.prod(shape)\n",
        "            tangents.append(tf.reshape(self.p[start:start + size], shape))\n",
        "            start += size\n",
        "        # self.gvp = tf.add_n([tf.reduce_sum(g * tangent) for (g, tangent) in zip(grads, tangents)])\n",
        "        self.gvp = [(tf.reduce_sum(g * t)) for (g, t) in zip(self.grads, tangents)]\n",
        "        # 2nd gradient of KL w/ itself * tangent\n",
        "\n",
        "        self.hvp = flatgrad(self.gvp, self.net_params)\n",
        "\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "  def conjugate_gradient(self, f_Ax, b, cg_iters=5, residual_tol=1e-5):\n",
        "        p = b.copy()\n",
        "        r = b.copy()\n",
        "        x = np.zeros_like(b)\n",
        "        rdotr = r.dot(r)\n",
        "        for i in range(cg_iters):\n",
        "            z = f_Ax(p)\n",
        "            v = rdotr / p.dot(z) # p.dot(z)  # stepdir size?? =ak of wikipedia\n",
        "            x += np.dot(v,p)\n",
        "            # x += v * p  # new parameters??\n",
        "            r -= z.dot(v)  # new gradient??\n",
        "            newrdotr = np.dot(r, r)  #\n",
        "            if newrdotr < residual_tol:\n",
        "                break\n",
        "\n",
        "            mu = newrdotr / rdotr  # Bi of wikipedia\n",
        "            rdotr = newrdotr\n",
        "            p = r + mu * p\n",
        "\n",
        "        return x\n",
        "\n",
        "  def linesearch(self, f, x, fullstepdir, expected_improve_rate, max_iter=5):\n",
        "        '''\n",
        "        :param f: loss fuction\n",
        "        :param x: parameters\n",
        "        :param fullstepdir: value returned by conjugate gradient * Hg-1 ... delta kappa estimated by the conjugate gradient\n",
        "        :param expected_improve_rate:\n",
        "        :return:\n",
        "        '''\n",
        "        j = max_iter\n",
        "        accept_ratio = .1\n",
        "        max_backtracks = 10\n",
        "\n",
        "        fval = f(x)\n",
        "        for (_n_backtracks, stepdirfrac) in enumerate(.5 ** np.arange(max_backtracks)):\n",
        "            j -= 1\n",
        "            xnew = x + (stepdirfrac * fullstepdir)\n",
        "            newfval = f(xnew)\n",
        "            actual_improve = fval - newfval\n",
        "            expected_improve = expected_improve_rate * stepdirfrac\n",
        "            ratio = actual_improve / expected_improve\n",
        "            if ratio > accept_ratio and actual_improve > 0 or j == 0:\n",
        "                return xnew\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "  def predict(self, sess, states):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          sess: TensorFlow session\n",
        "          states: array of states for which we want to predict the actions.\n",
        "        Returns:\n",
        "          The prediction of the output tensor.\n",
        "        \"\"\"\n",
        "        if states[-1].shape == (1,):\n",
        "            if len(states) == 3:\n",
        "                states = np.array(np.ravel(states))\n",
        "            else:\n",
        "                states = states[:-3]\n",
        "\n",
        "        states = np.atleast_2d(states)\n",
        "        np.reshape(states, [len(states), 3])\n",
        "        # print(states.shape)\n",
        "        feed = {self.inp: states}\n",
        "        prediction = sess.run(self.scaled_out, feed)\n",
        "\n",
        "        return prediction\n",
        "\n",
        "    # action gradient to be fed\n",
        "\n",
        "  def update(self, sess, states, actions, advantages, summary, first):\n",
        "        \"\"\"\n",
        "        Updates the weights of the neural network, based on its targets, its\n",
        "        predictions, its loss and its optimizer.\n",
        "\n",
        "        Args:\n",
        "          sess: TensorFlow session.\n",
        "          states: [current_state] or states of batch\n",
        "          actions: [current_action] or actions of batch\n",
        "          targets: [current_target] or targets of batch\n",
        "        \"\"\"\n",
        "        states = np.atleast_2d(states)\n",
        "        states = np.reshape(states, [len(states), 3])\n",
        "\n",
        "        #feed_dict = {self.inp: states, self.actions: actions}\n",
        "        #mean, sigma, scaled_out = sess.run((self.mean, self.sigma, self.scaled_out), feed_dict)\n",
        "\n",
        "        feed_dict = {self.inp: states, self.actions: actions,\n",
        "                     self.old_mean: self.prev_mean, self.old_sigma: self.prev_sigma,\n",
        "                     self.advantage: advantages}\n",
        "\n",
        "        self.prev_mean, self.prev_sigma,_, _, net, grads = sess.run(\n",
        "                    (self.mean, self.sigma, self.scaled_out, self.cost, self.net_params, self.grads), feed_dict)\n",
        "\n",
        "        grads = np.concatenate([np.reshape(grad, [np.size(v)]) for (v, grad) in zip(net, grads)], 0)\n",
        "        grads = np.where(np.isnan(grads), 1e-16, grads)\n",
        "\n",
        "        #self.sff = SetFromFlat(sess, net)\n",
        "\n",
        "        def get_hvp(p):\n",
        "            feed_dict[self.p] = p  # np.reshape(p, [np.size(p),1])\n",
        "            gvp = sess.run(self.gvp, feed_dict)\n",
        "            gvp = np.where(np.isnan(gvp), 0, gvp)\n",
        "            #with tf.control_dependencies(self.gvp):\n",
        "            a = tf.gradients(gvp, self.net_params)\n",
        "            a = [0 if k is None else  k for k in a]\n",
        "#            a = np.concatenate([np.reshape(grad, [np.size(v)]) for (v, grad) in zip(net, a)], 0)\n",
        "\n",
        "            return np.sum((1e-3 * np.reshape(p, [np.size(p), 1])) + np.reshape(a, [1, np.size(a)]), 1)\n",
        "\n",
        "            # return np.array(flatgrad(self.gvp, self.net_params))# + 1e-3 * p\n",
        "        \n",
        "        self.cg = self.conjugate_gradient(get_hvp, -grads)\n",
        "        self.stepdir = np.sqrt(2 * self.learning_rate / (np.transpose(grads) * self.cg) + 1e-16) * self.cg\n",
        "\n",
        "        def loss(th):\n",
        "            #th = np.concatenate([np.reshape(g,[-1]) for g in th],0)\n",
        "            #self.sff(th)\n",
        "            start = 0\n",
        "            i = 0\n",
        "            for (shape, v) in zip(self.shapes, self.net_params):\n",
        "                size = np.prod(shape)\n",
        "                self.net_params[i] = tf.reshape(th[start:start + size], shape)\n",
        "                start += size\n",
        "                i += 1\n",
        "            # surrogate loss: policy gradient loss\n",
        "            return sess.run(self.cost, feed_dict)\n",
        "\n",
        "        stepsize = self.linesearch(loss, np.concatenate([np.reshape(g,[-1]) for g in net],0), self.stepdir, self.cg.dot(self.stepdir))\n",
        "        #del self.sff\n",
        "        # self.net_params = sess.run(tf.assign(self.net_params, self.net_params + self.stepdir))#+ self.stepdir)# * stepsize\n",
        "        #+ self.stepdir)# * stepsize\n",
        "        for i, v in enumerate(self.net_params):\n",
        "            try:\n",
        "                for k in range(len(v)):\n",
        "                    self.net_params[i][k] += self.stepdir[i][k] * self.net_params[i][k]\n",
        "            except:\n",
        "                self.net_params[i] += self.stepdir[i] * self.net_params[i]\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ojrVjOYYj9y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "ac_dim = 1\n",
        "\n",
        "class Critic_Net():\n",
        "  def __init__(self, num_actions, action_dim, name, action_bound, state_dim, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.name = name\n",
        "        self.action_bound = action_bound\n",
        "        self.action_dim = action_dim\n",
        "        self.state_dim = state_dim\n",
        "        self._build_model(num_actions)\n",
        "\n",
        "  def _build_model(self, num_actions):\n",
        "\n",
        "        self.action = tf.placeholder(dtype=tf.float32, shape=[None, self.action_dim])\n",
        "        self.inp = tf.placeholder(shape=[None, self.state_dim], dtype=tf.float32)\n",
        "\n",
        "        self.inp_act = tf.concat([self.inp, self.action], 1)\n",
        "\n",
        "        self.inpW = tf.Variable(tf.random_uniform([self.state_dim +  self.action_dim, 16], -0.5, 0.5))\n",
        "        self.inpB = tf.Variable(tf.constant(0.1, shape=[16]))\n",
        "        self.h1 = tf.nn.relu(tf.matmul(self.inp_act, self.inpW) + self.inpB)\n",
        "\n",
        "        self.h2W = tf.Variable(tf.random_uniform([16, 32], -0.5, 0.5))\n",
        "        self.h2B = tf.Variable(tf.constant(0.1, shape=[32]))\n",
        "        self.h2 = tf.nn.relu(tf.matmul(self.h1, self.h2W) + self.h2B)\n",
        "\n",
        "        self.h3W = tf.Variable(tf.random_uniform([32, 16], -0.5, 0.5))\n",
        "        self.h3B = tf.Variable(tf.constant(0.1, shape=[16]))\n",
        "        self.h3 = tf.nn.relu(tf.matmul(self.h2, self.h3W) + self.h3B)\n",
        "\n",
        "        self.h4W = tf.Variable(tf.random_uniform([16, self.action_dim], -0.5, 0.5))\n",
        "\n",
        "        self.outB = tf.Variable(tf.constant(0.01, shape=[self.action_dim]))\n",
        "        self.outputs = tf.nn.relu(tf.matmul(self.h3, self.h4W) + self.outB)\n",
        "\n",
        "        # self.out = np.rint(self.out) # round to 0 or 1 as out\n",
        "        self.y_ = tf.placeholder(shape=[None, self.action_dim], dtype=tf.float32)\n",
        "\n",
        "        #        Q_grad = K.gradients(Q_pred, actions)\n",
        "\n",
        "        self.trainer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "        self.loss = tf.reduce_mean(tf.squared_difference(self.outputs, self.y_))\n",
        "\n",
        "        self.step = self.trainer.minimize(self.loss)\n",
        "\n",
        "        self.action_grads = tf.gradients(self.outputs, self.action)\n",
        "\n",
        "        self.saver = tf.train.Saver()\n",
        "  def predict(self, sess, states, actions):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          sess: TensorFlow session\n",
        "          states: array of states for which we want to predict the actions.\n",
        "        Returns:\n",
        "          The prediction of the output tensor.\n",
        "        \"\"\"\n",
        "\n",
        "        states = np.atleast_2d(states)\n",
        "        states = np.reshape(states, [len(states), 3])\n",
        "\n",
        "        feed = {self.inp: states, self.action: actions}\n",
        "        prediction = sess.run(self.outputs, feed)\n",
        "\n",
        "        return prediction\n",
        "\n",
        "  def update(self, sess, states, actions, targets, summary):\n",
        "        \"\"\"\n",
        "        Updates the weights of the neural network, based on its targets, its\n",
        "        predictions, its loss and its optimizer.\n",
        "\n",
        "        Args:\n",
        "          sess: TensorFlow session.\n",
        "          states: [current_state] or states of batch\n",
        "          actions: [current_action] or actions of batch\n",
        "          targets: [current_target] or targets of batch\n",
        "        \"\"\"\n",
        "\n",
        "        #pred = self.predict(sess, states, actions)\n",
        "\n",
        "\n",
        "        states = np.atleast_2d(states)\n",
        "        states = np.reshape(states, [len(states), 3])\n",
        "        return sess.run(self.loss, feed_dict={self.inp: states,self.action: actions, self.y_: targets})\n",
        "\n",
        "\n",
        "  def action_gradients(self, sess, states, actions):\n",
        "        return sess.run(self.action_grads, feed_dict={\n",
        "            self.inp: states,\n",
        "            self.action: actions})\n",
        "\n",
        "\n",
        "class Critic_Target_Network(Critic_Net):\n",
        "    \"\"\"\n",
        "    Slowly updated target network. Tau indicates the speed of adjustment. If 1,\n",
        "    it is always set to the values of its associate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_actions, action_dim, name, action_bound, state_dim, learning_rate=0.001, tau=0.001):\n",
        "        super().__init__(num_actions, action_dim, name, action_bound, state_dim, learning_rate)\n",
        "        self.tau = tau\n",
        "        self._associate = self._register_associate()\n",
        "\n",
        "    def _register_associate(self):\n",
        "\n",
        "        critic_vars =tf.trainable_variables(\"critic\")#\"critic\"\n",
        "        target_vars =tf.trainable_variables(\"critic_target\")#\"critic_target\"\n",
        "\n",
        "        op_holder = []\n",
        "        for idx, var in enumerate(target_vars):  # // is to retun un integer\n",
        "            op_holder.append(var.assign(\n",
        "                (critic_vars[idx].value() * self.tau) + ((1 - self.tau) * var.value())))\n",
        "        #return target_vars.assign((critic_vars * self.tau )+((1 - self.tau) * target_vars))\n",
        "        return op_holder\n",
        "\n",
        "    def update(self, sess):\n",
        "        for op in self._associate:\n",
        "            sess.run(op)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ykHC-zByZ3bz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L6Ccei0DYzkp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "\n",
        "from os import path\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class PendulumEnv(gym.Env):\n",
        "    metadata = {\n",
        "        'render.modes': ['human', 'rgb_array'],\n",
        "        'video.frames_per_second': 30\n",
        "    }\n",
        "\n",
        "    def __init__(self, reward_function=None):\n",
        "        self.max_speed = 8\n",
        "        self.max_torque = 2.\n",
        "        self.dt = .05\n",
        "        self.viewer = None\n",
        "\n",
        "        high = np.array([1., 1., self.max_speed])\n",
        "        self.action_space = spaces.Box(low=-self.max_torque, high=self.max_torque, shape=(1,))\n",
        "        self.observation_space = spaces.Box(low=-high, high=high)\n",
        "\n",
        "        self._seed()\n",
        "\n",
        "        if reward_function is None:\n",
        "            def reward(pendulum):\n",
        "                return 1 if -0.1 <= angle_normalize(pendulum.state[0]) <= 0.1 else 0\n",
        "\n",
        "            self.reward = reward\n",
        "        else:\n",
        "            self.reward = reward_function\n",
        "\n",
        "    def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def step(self, u):\n",
        "        th, thdot = self.state\n",
        "\n",
        "        g = 10.\n",
        "        m = 1.\n",
        "        l = 1.\n",
        "        dt = self.dt\n",
        "\n",
        "        u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
        "\n",
        "        self.last_u = u\n",
        "        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt\n",
        "        newth = th + newthdot * dt\n",
        "        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)\n",
        "        reward = self.reward(self)\n",
        "\n",
        "        self.state = np.array([newth, newthdot])\n",
        "\n",
        "        return self._get_obs(), reward, False, {}\n",
        "\n",
        "    def reset(self):\n",
        "        high = np.array([np.pi, 1])\n",
        "        self.state = self.np_random.uniform(low=-high, high=high)\n",
        "        self.last_u = None\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        theta, thetadot = self.state\n",
        "        return np.array([np.cos(theta), np.sin(theta), thetadot])\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            if self.viewer is not None:\n",
        "                self.viewer.close()\n",
        "                self.viewer = None\n",
        "            return\n",
        "\n",
        "        if self.viewer is None:\n",
        "            from gym.envs.classic_control import rendering\n",
        "            self.viewer = rendering.Viewer(500, 500)\n",
        "            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)\n",
        "            rod = rendering.make_capsule(1, .2)\n",
        "            rod.set_color(.8, .3, .3)\n",
        "            self.pole_transform = rendering.Transform()\n",
        "            rod.add_attr(self.pole_transform)\n",
        "            self.viewer.add_geom(rod)\n",
        "            axle = rendering.make_circle(.05)\n",
        "            axle.set_color(0, 0, 0)\n",
        "            self.viewer.add_geom(axle)\n",
        "            fname = path.join(path.dirname(__file__), \"assets/clockwise.png\")\n",
        "            self.img = rendering.Image(fname, 1., 1.)\n",
        "            self.imgtrans = rendering.Transform()\n",
        "            self.img.add_attr(self.imgtrans)\n",
        "\n",
        "        self.viewer.add_onetime(self.img)\n",
        "        self.pole_transform.set_rotation(self.state[0] + np.pi / 2)\n",
        "        if self.last_u:\n",
        "            self.imgtrans.scale = (-self.last_u / 2, np.abs(self.last_u) / 2)\n",
        "\n",
        "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n",
        "\n",
        "\n",
        "def angle_normalize(x):\n",
        "    if np.isnan(x):\n",
        "        print(\"ISNOTNUMBER\")\n",
        "    val = (((x + np.pi) % (2 * np.pi)) - np.pi)\n",
        "    return val\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "crBLdXywY_wc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "47c293ea-0388-44b3-dbea-a55329a8d9b6"
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  \n",
        "    print(\"start\")\n",
        "    env = PendulumEnv()\n",
        "    action_space = np.arange(-2, 2.01, 0.01)\n",
        "    num_actions = len(action_space)\n",
        "    action_dim = 1\n",
        "    action_bound = env.action_space.high\n",
        "    state_dim = 3\n",
        "    batch_size = 32\n",
        "    learning_rate = 0.001\n",
        "    delta = 0.01\n",
        "    discount_factor = 0.99\n",
        "    num_episodes = 500\n",
        "    len_episode = 100\n",
        "    epsilon = 0.1\n",
        "    load = False\n",
        "    if not load:\n",
        "\n",
        "        g_stat = []\n",
        "\n",
        "        config = tf.ConfigProto()\n",
        "        \n",
        "        config.gpu_options.allow_growth = True\n",
        "\n",
        "        with tf.Session(config=config)as sess:\n",
        "\n",
        "            with tf.name_scope(\"actor\"):\n",
        "                actor = Actor_Net(num_actions, action_dim, \"actor\", action_bound, state_dim,\n",
        "                                  learning_rate=learning_rate)\n",
        "\n",
        "            with tf.name_scope(\"critic\"):\n",
        "                critic = Critic_Net(num_actions, action_dim, \"critic\", action_bound, state_dim,\n",
        "                                    learning_rate=learning_rate)\n",
        "\n",
        "\n",
        "            writer = tf.summary.FileWriter('./TRPO/TRPO_loss', sess.graph)\n",
        "            #summ_critic_loss = tf.summary.scalar('loss_critic', critic.get_loss())\n",
        "\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            g = sess.graph\n",
        "            \"\"\"\n",
        "            Trpo\n",
        "            \"\"\"\n",
        "            loss_episodes = []\n",
        "            stats = EpisodeStats(episode_lengths=np.zeros(num_episodes), episode_rewards=np.zeros(num_episodes))\n",
        "            buffer = ReplayBuffer()\n",
        "            nTimes_actions = np.ones(num_actions)\n",
        "\n",
        "            for i_episode in range(num_episodes):\n",
        "                loss = []\n",
        "                # Also print reward for last episode\n",
        "                last_reward = stats.episode_rewards[i_episode - 1]\n",
        "                print(\"\\rEpisode {}/{} ({})\".format(i_episode + 1, num_episodes, last_reward), end=\"\")\n",
        "                sys.stdout.flush()\n",
        "\n",
        "                done = False\n",
        "                i = 0\n",
        "                g_r = 0\n",
        "\n",
        "                observation = env.reset()\n",
        "\n",
        "                while not done and i < len_episode:\n",
        "\n",
        "                    first = True\n",
        "                    if i != 0:\n",
        "                        first = False\n",
        "                        sess.graph.clear_collection(\"theta_sff\")\n",
        "                    loss = []\n",
        "                    i += 1\n",
        "                    old_observation = observation\n",
        "                    action = np.take(actor.predict(sess, observation), [0])\n",
        "\n",
        "                   # env.render()\n",
        "                    observation, reward, done, info = env.step([action])\n",
        "\n",
        "                    buffer.add_transition(old_observation, action, observation, reward, done)\n",
        "                    s, a, ns, r, d = buffer.next_batch(batch_size)\n",
        "\n",
        "                    pred_actions = actor.predict(sess, ns)\n",
        "\n",
        "                    q_values = critic.predict(sess, ns, pred_actions)\n",
        "\n",
        "                    r = np.reshape(r,[-1,1])\n",
        "                    y = q_values - r\n",
        "\n",
        "                    g_r += reward\n",
        "                    g_stat.append(int(np.round(g_r)))\n",
        "\n",
        "                    loss_critic = critic.update(sess, s, a, y, None)\n",
        "\n",
        "                    loss.append(loss_critic)\n",
        "\n",
        "                    sys.stdout.flush()\n",
        "\n",
        "                    actor.update(sess, s, a, y, None, first)\n",
        "\n",
        "                    stats.episode_rewards[i_episode] += reward\n",
        "\n",
        "                    g_stat.append(int(np.round(g_r)))\n",
        "\n",
        "                    #sess.graph.as_default()\n",
        "\n",
        "                l = sum(loss)\n",
        "                summ_critic_loss = tf.Summary(value=[tf.Summary.Value(tag=\"loss_critic\",\n",
        "                                                                      simple_value=l)])\n",
        "                writer.add_summary(summ_critic_loss, i_episode)\n",
        "\n",
        "                writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag=\"episode_rewards\",\n",
        "                                                                     simple_value=stats.episode_rewards[i_episode])]), i_episode)\n",
        "\n",
        "\n",
        "                writer.flush()\n",
        "                loss_episodes.append(l)\n",
        "\n",
        "                stats.episode_lengths[i_episode] = i\n",
        "\n",
        "                gc.collect()\n",
        "                #tf.reset_default_graph()\n",
        "\n",
        "                #tf.get_default_graph().finalize()\n",
        "            plot_episode_stats(stats)\n",
        "            plot_stats(loss_episodes)\n",
        "            # return stats, loss_episodes\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start\n",
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "Episode 1/500 (0.0)"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:193: RuntimeWarning: divide by zero encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:193: RuntimeWarning: invalid value encountered in sqrt\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:193: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}